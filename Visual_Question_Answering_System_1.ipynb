{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbJdTXDGSFZe"
      },
      "source": [
        "# Application #1 -VQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjBRrsCdKvm0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "y2Hjveq-JLVK"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from transformers import ViltProcessor, ViltForQuestionAnswering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ELZmBJw3Klne"
      },
      "outputs": [],
      "source": [
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(image, text):\n",
        "    try:\n",
        "        # Load and process the image\n",
        "        img = Image.open(BytesIO(image)).convert(\"RGB\")\n",
        "\n",
        "        # Prepare inputs\n",
        "        encoding = processor(img, text, return_tensors=\"pt\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**encoding)\n",
        "        logits = outputs.logits\n",
        "        idx = logits.argmax(-1).item()\n",
        "        answer = model.config.id2label[idx]\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ],
      "metadata": {
        "id": "v3nlDEn9AKLb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Xyv4medPQQRi"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"swim.jpg\")\n",
        "image_byte_array = BytesIO()\n",
        "image.save(image_byte_array, format='JPEG')\n",
        "image_bytes = image_byte_array.getvalue()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ques = \"what the man is doing?\"\n",
        "get_answer(image_bytes,ques)"
      ],
      "metadata": {
        "id": "W-xAj1UtAv5p",
        "outputId": "f284389a-8091-40cb-b448-238e86ca8dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'swimming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Br-Iv9VSaX6"
      },
      "source": [
        "# Application #2 -- img2text using pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract pillow pdf2image\n"
      ],
      "metadata": {
        "id": "K9Mv9f7UTWJ0",
        "outputId": "9180ea5e-54fd-4e92-ed14-87e4d39d7885",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# Install Tesseract in Colab\n",
        "!sudo apt install tesseract-ocr\n",
        "\n",
        "# Path to the Tesseract executable in Colab\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "\n"
      ],
      "metadata": {
        "id": "1uBieqolTl3B",
        "outputId": "864dd5f3-57e8-4843-fd6e-fc887da1c517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (29.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120511 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the uploaded image using PIL\n",
        "image = Image.open('text.png')\n",
        "\n",
        "# Perform OCR on the image\n",
        "text = pytesseract.image_to_string(image)\n",
        "\n",
        "# Print the extracted text\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "0OuFpX2JUXPm",
        "outputId": "8e3fae91-3e8f-438c-a271-45fb243be1df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freeman 1\n",
            "\n",
            "Brandon Freeman:\n",
            "Professor Lee\n",
            "English 101\n",
            "25 February 2017\n",
            "\n",
            "Problems with Assisted Reproductive Technology and the Definition of the Family\n",
            "\n",
            "It is not unusual for people to think of a family in its basic form as a mother and a\n",
            "\n",
            "father and the child or children they conceive together. But a genetic connection\n",
            "between parents and children is not necessary for a family to exist. New families are\n",
            "often created by remarriage after a divorce or the death of a spouse, so that only one\n",
            "parent is genetically related to the child or children. Also, the practice of adoption is\n",
            "long-standing and creates families where neither parent is genetically related to the\n",
            "child or children. There are many single-parent families in the United States, and some\n",
            "of these may be families where the parents live together but are not married (Coontz\n",
            "\n",
            "147). Couples that consist of two men or two women are also increasingly common, and\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF to JPEG to TEXT"
      ],
      "metadata": {
        "id": "BL1Wy6xvfj83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# Path to the Tesseract executable in Colab\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "\n"
      ],
      "metadata": {
        "id": "1uu77T8-fI4o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n"
      ],
      "metadata": {
        "id": "CL0BsYQAhzIt",
        "outputId": "07ebfd81-ee13-432b-9339-0732f1f2a2fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n",
            "Fetched 186 kB in 1s (368 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120558 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Upload the PDF file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Convert PDF pages to images (JPEG format)\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "images = convert_from_path(pdf_path)\n",
        "\n",
        "# Loop through each image and perform OCR\n",
        "for i, image in enumerate(images):\n",
        "    # Perform OCR on the image\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    print(f\"Extracted text from page {i + 1}:\\n{text}\\n\")"
      ],
      "metadata": {
        "id": "Y5FavhGIffgg",
        "outputId": "98cea8f6-1a19-4b4c-fbc9-bc9f29ea8223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-909451e1-942c-4bf4-8d27-b14b99059dcb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-909451e1-942c-4bf4-8d27-b14b99059dcb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving trial.pdf to trial (2).pdf\n",
            "Extracted text from page 1:\n",
            "The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n",
            "\n",
            "A Semi-supervised Learning Approach with Two Teachers\n",
            "to Improve Breakdown Identification in Dialogues\n",
            "\n",
            "Qian Lin, Hwee Tou Ng\n",
            "\n",
            "Department of Computer Science, National University of Singapore\n",
            "qlin@u.nus.edu, nght@comp.nus.edu.sg\n",
            "\n",
            "Abstract\n",
            "\n",
            "Identifying breakdowns in ongoing dialogues helps to improve\n",
            "communication effectiveness. Most prior work on this topic\n",
            "relies on human annotated data and data augmentation to learn\n",
            "a classification model. While quality labeled dialogue data re-\n",
            "quires human annotation and is usually expensive to obtain, un-\n",
            "labeled data is easier to collect from various sources. In this pa-\n",
            "per, we propose a novel semi-supervised teacher-student learn-\n",
            "ing framework to tackle this task. We introduce two teachers\n",
            "which are trained on labeled data and perturbed labeled data\n",
            "respectively. We leverage unlabeled data to improve classifica-\n",
            "tion in student training where we employ two teachers to refine\n",
            "the labeling of unlabeled data through teacher-student learn-\n",
            "ing in a bootstrapping manner. Through our proposed training\n",
            "approach, the student can achieve improvements over single-\n",
            "teacher performance. Experimental results on the Dialogue\n",
            "Breakdown Detection Challenge dataset DBDCS5 and Learning\n",
            "to Identify Follow-Up Questions dataset LIF show that our ap-\n",
            "proach outperforms all previous published approaches as well\n",
            "as other supervised and semi-supervised baseline methods.\n",
            "\n",
            "Introduction\n",
            "\n",
            "In recent years, interactive virtual conversational agents have\n",
            "been developed rapidly and used widely in daily lives. The\n",
            "information exchange between a user and an agent is done\n",
            "via a conversational dialogue. To achieve effective communi-\n",
            "cation, the agent is expected to generate a proper and rational\n",
            "response based on not only the last turn but also all previous\n",
            "utterances in the dialogue history to continue the dialogue.\n",
            "The user’s trust in the agent is damaged when the agent fails\n",
            "to identify the user’s intent and generates an inappropriate\n",
            "response, which confuses the user and causes a breakdown in\n",
            "the dialogue. Therefore, identifying breakdowns in dialogues\n",
            "is essential for improving the effectiveness of conversational\n",
            "agents, so that the agent is able to avoid generating responses\n",
            "which cause the breakdowns.\n",
            "\n",
            "Much prior work on breakdown identification in dialogues\n",
            "has focused on supervised learning on human annotated\n",
            "data. One line of work relies on feature-engineered ma-\n",
            "chine learning methods including decision trees and ran-\n",
            "dom forests (Wang, Kato, and Sakai 2019). Another line\n",
            "of work utilizes non-Transformer based neural networks\n",
            "\n",
            "Copyright © 2022, Association for the Advancement of Artificial\n",
            "Intelligence (www.aaai.org). All rights reserved.\n",
            "\n",
            "11011\n",
            "\n",
            "such as LSTM (Hendriksen, Leeuwenberg, and Moens 2019;\n",
            "Wang, Kato, and Sakai 2019; Shin, Dirafzoon, and Anshu\n",
            "2019). Transformer-based methods involve pre-trained lan-\n",
            "guage models which are pre-trained on large corpora (Devlin\n",
            "et al. 2019; Conneau et al. 2020). Sugiyama (2019) and utilize\n",
            "BERT with input consisting of the text and textual features\n",
            "from the dialogue. Lin, Kundu, and Ng (2020) introduce mul-\n",
            "tilingual transfer learning through a cross-lingual pre-trained\n",
            "language model and co-attention modules to reason between\n",
            "the dialogue history and the last utterance.\n",
            "\n",
            "A recent work (Ng et al. 2020) proposes to perform pre-\n",
            "training on BERT with conversational data and apply self-\n",
            "supervised data augmentation on labeled data. Although good\n",
            "performance has been reported, we observe that the gain of\n",
            "either continued pre-training or data augmentation on labeled\n",
            "data is marginal over the conventional BERT classification\n",
            "scheme. Moreover, pre-training of a pre-trained language\n",
            "model on large corpora is resource-intensive.\n",
            "\n",
            "We believe that training with dialogue data from other\n",
            "sources introduces diversity and enables the trained model to\n",
            "generalize better. Since annotated dialogue data is expensive\n",
            "to obtain, we propose using unlabeled data through semi-\n",
            "supervised learning and self-training, such that the training\n",
            "data is enriched and more diverse.\n",
            "\n",
            "In this work!, we propose a novel semi-supervised teacher-\n",
            "student learning framework to improve the performance of\n",
            "pre-trained language models with unlabeled data. We lever-\n",
            "age unlabeled data from other sources to enrich the training\n",
            "set through self-training, which is a general case of domain\n",
            "adaptation where source data and target data are sampled\n",
            "from different data sources. Self-training uses a trained clas-\n",
            "sifier to assign label score vectors on unlabeled data instances.\n",
            "However, such labeling process tends to generate labels under\n",
            "the assumption that a similar distribution is shared by labeled\n",
            "data and unlabeled data. Since the distribution of unlabeled\n",
            "data is difficult to estimate, we introduce two teachers to\n",
            "improve the labeling of unlabeled data. The student model is\n",
            "encouraged to integrate the knowledge from two teachers in\n",
            "a bootstrapping manner.\n",
            "\n",
            "We leverage a data augmentation method (Yavuz et al.\n",
            "2020) incorporating [MASK] tokens derived from a Masked\n",
            "\n",
            "'The source code and trained models of this paper are available\n",
            "at https://github.com/nusnlp/S2T2.\n",
            "\f\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}